input {
  jdbc {
    #Database connetion
    jdbc_driver_library => "/usr/share/java/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/fts"
    jdbc_user => "fts"
    jdbc_password => "fts"
    id => "backup"

    #Fields to control which data is taken
    #Activate clean_run will load all DB data
    clean_run => "true"
    record_last_run => "true"
    tracking_column => "job_finished"
    tracking_column_type => "timestamp"
    use_column_value => "true"
    last_run_metadata_path => "/etc/logstash/jdbc_metadata/.logstash_jdbs_last_run" 
    jdbc_fetch_size => "1000"

    #Rufus Scheduler
    schedule => "* * * * *"
    #Every five minutes
    #schedule => "/5 * * * *"

    statement => "select f.job_id, f.filesize, f.tx_duration, f.throughput, f.user_filesize, f.transferred, j.source_se, f.source_surl, j.dest_se, j.user_dn, j.vo_name, j.job_finished, j.job_state, f.file_state, j.space_token from t_file_backup f, t_job_backup j where f.job_id = j.job_id and j.job_finished > :sql_last_value order by j.job_finished"
  }

 
jdbc {
    #Database connetion
    jdbc_driver_library => "/usr/share/java/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/fts"
    jdbc_user => "fts"
    jdbc_password => "fts"
    id => "main"

    #Fields to control which data is taken
    #Activate clean_run will load all DB data
    clean_run => "true"
    record_last_run => "true"
    tracking_column => "job_finished"
    tracking_column_type => "timestamp"
    use_column_value => "true"
    last_run_metadata_path => "/etc/logstash/jdbc_metadata/.main_jdbc_last_run"

    #Rufus Scheduler
    schedule => "* * * * *"
    #Every five minutes
    #schedule => "/5 * * * *"

    statement => "select f.job_id, f.filesize, f.tx_duration, f.throughput, f.user_filesize, f.transferred, j.source_se, f.source_surl, j.dest_se, j.user_dn, j.vo_name, j.job_finished, j.job_state, f.file_state, j.space_token from t_file f, t_job j where f.job_id = j.job_id and j.job_finished > :sql_last_value order by j.job_finished"
    }
}

filter {
 grok{
  patterns_dir => ["./patterns"]
  match => { "user_dn" => "%{DATA}CN=%{DATA:user} %{EMAILADDRESS:email}%{DATA}" }
  remove_field => [ "user_dn" ]
  tag_on_failure => ["_userdnparsefailure"]
 }

 grok{
  patterns_dir => ["./patterns"]
  match => { "job_finished" => "%{TIMESTAMP_ISO8601:tstamp}"}
  remove_field => ["job_finished"]
  tag_on_failure => ["_tstampparsefailure"]
 }
 grok{
  patterns_dir => ["./patterns"]
  match => { "dest_se" => "%{DATA}//%{USER:destination}" }
  remove_field => [ "dest_se" ]
  tag_on_failure => ["_destinationparsefailure"]
 }
 grok{
  match => { "source_se" => "%{DATA}//%{USER:source}" }
  remove_field => [ "source_se" ]
  tag_on_failure => ["_sourceparsefailure"]
 }
 grok{
    match => { "source_surl" => "/data/%{DATA:Categoria}/%{DATA:Categoria_2}/%{DATA:Categoria_3}/" }
  remove_field => [ "source_surl" ]
  tag_on_failure => ["_sourceurlparsefailure"]
 }

 date {
  match => [ "tstamp", "ISO8601" ]
  target => "@timestamp"
 }

 mutate{
  remove_field => ["tstamp"]
 }
}

output {
    #stdout {}
    elasticsearch {
        index => "fts-%{+YYYY.MM.dd}"
        document_id => "%{job_id}"
        hosts => [ "localhost:9200" ]
    }
}



