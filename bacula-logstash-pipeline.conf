################# File input, must delete sinceBD file to load same data twice or more times ###############
input {
  file {
    path => "/etc/logstash/pipes/*.log"
    start_position => "beginning"
    #ignore_older => 0 
    file_completed_action => "log"
    file_completed_log_path => "/etc/logstash/pipes/log_paths"
    mode => "read"
    delimiter => "End auto prune."
  }
}

############ Parse every parameter of the log string #############

filter {
 grok{
  patterns_dir => ["./patterns"]
  match => { "message" => "Build OS:\s*%{DATA:Build_OS}\n"}
  tag_on_failure => ["_Build_failure"]
 }
 grok{
  patterns_dir => ["./patterns"]
  tag_on_failure => ["_jobid_failure"]
  match => { "message" => "JobId:\s*%{DATA:JobId}\n"}
 }
 grok{
  patterns_dir => ["./patterns"]
  tag_on_failure => ["_job_failure"]
  match => { "message" => "Job:\s*%{DATA:Job}\n"}
 }
 grok{
  patterns_dir => ["./patterns"]
  match => { "message" => "Backup\sLevel:\s*%{DATA:Backup_Level}\n"}
  tag_on_failure => ["_backupLevel_failure"]
 }
 grok{
  patterns_dir => ["./patterns"]
  match => { "message" => "Client:\s*%{DATA:Client}\n"}
  tag_on_failure => ["_Client_failure"]
 }
 grok{
  patterns_dir => ["./patterns"]
  match => { "message" => "FileSet:\s*%{DATA:FileSet}\n"}
  tag_on_failure => ["_Fileset_failure"]
 }
 grok{
  patterns_dir => ["./patterns"]
  match => { "message" => "Pool:\s*%{DATA:Pool}\n"}
  tag_on_failure => ["_Pool_failure"]
 }
 grok{
  patterns_dir => ["./patterns"]
  match => { "message" => "Catalog:\s*%{DATA:Catalog}\n"}
  tag_on_failure => ["_catalog_failure"]
 }
 grok{
  patterns_dir => ["./patterns"]
  match => { "message" => "Storage:\s*%{DATA:Storage}\n"}
  tag_on_failure => ["_storage_failure"]
 }
 grok{
  patterns_dir => ["./patterns"]
  match => { "message" => "Scheduled\stime:\s*%{DATA:Scheduled_time}\n"}
  tag_on_failure => ["_scheduledTime_failure"]
 }
 grok{
  patterns_dir => ["./patterns"]
  match => { "message" => "Start\stime:\s*%{DATA:Start_time}\n"}
  tag_on_failure => ["_startTime_failure"]
 }
 grok{
  patterns_dir => ["./patterns"]
  match => { "message" => "End\stime:\s*%{DATA:End_time}\n"}
  tag_on_failure => ["_endTime_failure"]
 }
  grok{
  patterns_dir => ["./patterns"]
  match => { "message" => "Priority:\s*%{DATA:Priority}\n"}
  tag_on_failure => ["_priority_failure"]
 }
 grok{
  patterns_dir => ["./patterns"]
  match => { "message" => "FD\sFiles\sWritten:\s*%{DATA:FD_Files_Written}\n"}
  tag_on_failure => ["_fdFilesWritten_failure"]
 }
 grok{
  patterns_dir => ["./patterns"]
  match => { "message" => "SD\sFiles\sWritten:\s*%{DATA:SD_Files_Written}\n"}
  tag_on_failure => ["_SDFilesWritten_failure"]
 }
 grok{
  patterns_dir => ["./patterns"]
  match => { "message" => "FD\sBytes\sWritten:\s*%{DATA}\s\(%{DATA:FD_MBytes_Written}\)\n"}
  tag_on_failure => ["_FDbytesWritten_failure"]
 }
 grok{
  patterns_dir => ["./patterns"]
  match => { "message" => "SD\sBytes\sWritten:\s*%{DATA}\s\(%{DATA:SD_MBytes_Written}\)\n"}
  tag_on_failure => ["_SDBytesWritten_failure"]
 }
 grok{
  patterns_dir => ["./patterns"]
  match => { "message" => "Rate:\s*%{DATA:Rate}\n"}
  tag_on_failure => ["_rate_failure"]
 }
 grok{
  patterns_dir => ["./patterns"]
  match => { "message" => "Software\sCompression:\s*%{DATA:Software_Compression_Percentage}\%\s"}
  tag_on_failure => ["No_software_compression"]
 }
 grok{
  patterns_dir => ["./patterns"]
  match => { "message" => "Snapshot/VSS:\s*%{DATA:Snapshot}\n"}
  tag_on_failure => ["_Snapshot_failure"]
 }
 grok{
  patterns_dir => ["./patterns"]
  match => { "message" => "Encryption:\s*%{DATA:Encryption}\n"}
  tag_on_failure => ["_encryption_failure"]
 }
 grok{
  patterns_dir => ["./patterns"]
  match => { "message" => "Accurate:\s*%{DATA:Accurate}\n"}
  tag_on_failure => ["_accurate_failure"]
 }

 grok{
  patterns_dir => ["./patterns"]
  match => { "message" => "Volume\sname\(s\):\s*%{DATA:Volume_name}\n"}
  tag_on_failure => ["_Volume_name_failure"]
 }
 
 grok{
  patterns_dir => ["./patterns"]
  match => { "message" => "Volume\sSession\sId:\s*%{DATA:Volume_Session}\n"}
  tag_on_failure => ["_volumeSessionID_failure"]
 }
 
 grok{
  patterns_dir => ["./patterns"]
  match => { "message" => "Volume\sSession\sTime:\s*%{DATA:Volume_Session_Time}\n"}
  tag_on_failure => ["_volumeSessionTime_failure"]
 }
 
 grok{
  patterns_dir => ["./patterns"]
  match => { "message" => "Last\sVolume\sBytes:\s*%{DATA}\s\(%{DATA:Last_Volume_MBytes}\)\n"}
  tag_on_failure => ["_lastVolumeBytes_failure"]
 }
 
 grok{
  patterns_dir => ["./patterns"]
  match => { "message" => "Non-fatal\sFD\serrors:\s*%{DATA:Non-fatal_FD_errors}\n"}
  tag_on_failure => ["_NonFatalErrors_failure"]
 }
 
 grok{
  patterns_dir => ["./patterns"]
  match => { "message" => "SD\sErrors:\s*%{DATA:SD_Errors}\n"}
  tag_on_failure => ["_SDErrors_failure"]
 }

 grok{
  patterns_dir => ["./patterns"]
  match => { "message" => "FD\stermination\sstatus:\s*%{DATA:FD_termination_status}\n"}
  tag_on_failure => ["_FDterminationStatus_failure"]
 }
 grok{
  patterns_dir => ["./patterns"]
  match => { "message" => "SD\stermination\sstatus:\s*%{DATA:SD_termination_status}\n"}
  tag_on_failure => ["_SDterminationStatus_failure"]
 }
 grok{
  patterns_dir => ["./patterns"]
  match => { "message" => "Termination:\s*%{DATA:Termination}\n"}
  tag_on_failure => ["_termination_failure"]
 } 
 grok{
  patterns_dir => ["./patterns"]
  match => { "message" => "\sTransfer\srate=%{DATA:Transfer_rate}\s"}
  tag_on_failure => ["_TransferRate_failure"]
 }
 grok{
  patterns_dir => ["./patterns"]
  match => { "message" => "\sUsing\sDevice\s%{DATA:Write_Device}\sto\swrite.\n"}
  tag_on_failure => ["_Writedevice_failure"]
 }
 grok{
  patterns_dir => ["./patterns"]
  match => { "message" => "\sDespooling\s%{DATA:Despooled_bytes}\s...\n"}
  tag_on_failure => ["_Despooled_failure"]
 }

########### Elapsed time is represented by diferent time units, so we parse them and transform them in seconds #########
########### we must have in count every possible string to parse it correctly ###########

grok{
  patterns_dir => ["./patterns"]
  match => { "message" => "Elapsed\stime:\s*%{DATA:Elapsed_time}\n"}
  tag_on_failure => ["_elapsedTime_failure"]
 }

if "day" in [Elapsed_time]{
  mutate{
     add_tag => ["day_if"]
  }
  grok{
     patterns_dir => ["./patterns"]
     match => { "Elapsed_time" => "(?<elapsed_day>[0-9]*)\sday"}
     tag_on_failure => ["_elapsedday_failure_if3"]
  }
  grok{
     patterns_dir => ["./patterns"]
     match => { "Elapsed_time" => "\s(?<elapsed_hour>[0-9]*)\shour"}
     tag_on_failure => ["_elapsedhour_failure_if3"]
  }
  grok{
     patterns_dir => ["./patterns"]
     match => { "Elapsed_time" => "\s(?<elapsed_min>[0-9]*)\smin"}
     tag_on_failure => ["_elapsedmin_failure_if3"]
  }
  grok{
     patterns_dir => ["./patterns"]
     match => { "Elapsed_time" => "\s(?<elapsed_sec>[0-9]*)\ssec"}
     tag_on_failure => ["_elapsedsec_failure_if3"]
  }
  mutate{
    add_field => {"elapsed_time_secs" => 0}
    convert => {
       "elapsed_hour" => "integer"
       "elapsed_min" => "integer"
       "elapsed_sec" => "integer"
       "elapsed_day" => "integer"
       "elapsed_time_secs" => "integer"
    }
  }
  ruby {
    code => "event.set('elapsed_day', event.get('elapsed_day') * 60 * 60 * 24)"
  } 
  if "_elapsedhour_failure_if3" in [tags]{
    ruby {
      code => "event.set('elapsed_min', event.get('elapsed_min') * 60)"
    }
    ruby {
       code => "event.set('elapsed_time_secs', event.get('elapsed_min') + event.get('elapsed_sec') + event.get('elapsed_day'))"
    }
  }else if "_elapsedmin_failure_if3" in [tags]{
    ruby {
      code => "event.set('elapsed_hour', event.get('elapsed_hour') * 60 * 60)"
    }
    ruby {
       code => "event.set('elapsed_time_secs', event.get('elapsed_hour') + event.get('elapsed_sec') + event.get('elapsed_day'))"
    }
  }else if "_elapsedsec_failure_if3" in [tags]{
    ruby {
      code => "event.set('elapsed_min', event.get('elapsed_min') * 60)"
    }
    ruby {
      code => "event.set('elapsed_hour', event.get('elapsed_hour') * 60 * 60)"
    }
    ruby {
       code => "event.set('elapsed_time_secs', event.get('elapsed_min') + event.get('elapsed_hour') + event.get('elapsed_day'))"
    }
  }else{
    ruby {
      code => "event.set('elapsed_min', event.get('elapsed_min') * 60)"
    }
    ruby {
      code => "event.set('elapsed_hour', event.get('elapsed_hour') * 60 * 60)"
    }
    ruby {
       code => "event.set('elapsed_time_secs', event.get('elapsed_min') + event.get('elapsed_hour') + event.get('elapsed_sec') + event.get('elapsed_day'))"
    }
  }

}else if "hour" in [Elapsed_time]{
  mutate{
     add_tag => ["hour_if"]
  }
  grok{
     patterns_dir => ["./patterns"]
     match => { "Elapsed_time" => "(?<elapsed_hour>[0-9]*)\shour"}
     tag_on_failure => ["_elapsedhour_failure_if2"]
  }
  grok{
     patterns_dir => ["./patterns"]
     match => { "Elapsed_time" => "\s(?<elapsed_min>[0-9]*)\smin"}
     tag_on_failure => ["_elapsedmin_failure_if2"]
  }
  grok{
     patterns_dir => ["./patterns"]
     match => { "Elapsed_time" => "\s(?<elapsed_sec>[0-9]*)\ssec"}
     tag_on_failure => ["_elapsedsec_failure_if2"]
  }
  mutate{
    add_field => {"elapsed_time_secs" => 0}
    convert => {
       "elapsed_hour" => "integer"
       "elapsed_min" => "integer"
       "elapsed_sec" => "integer"
       "elapsed_time_secs" => "integer"
    }
  }
  if "_elapsedmin_failure_if2" in [tags]{ 
    ruby {
       code => "event.set('elapsed_time_secs', event.get('elapsed_sec') + event.get('elapsed_hour'))"
    }
  }else if "_elapsedsec_failure_if2" in [tags]{
    ruby {
       code => "event.set('elapsed_min', event.get('elapsed_min') * 60)"
    }
    ruby {
       code => "event.set('elapsed_time_secs', event.get('elapsed_min') + event.get('elapsed_hour'))"
    }
  }else{
    ruby {
      code => "event.set('elapsed_hour', event.get('elapsed_hour') * 60 * 60)"
    }
    ruby {
      code => "event.set('elapsed_min', event.get('elapsed_min') * 60)"
    }
    ruby{
       code => "event.set('elapsed_time_secs', event.get('elapsed_sec') + event.get('elapsed_min') + event.get('elapsed_hour'))"
    }
  }

}else if "min" in [Elapsed_time]{
  mutate{
    add_tag => ["min_if"]
  }
  grok{
     patterns_dir => ["./patterns"]
     match => { "Elapsed_time" => "%{DATA:elapsed_min}\smin"}
     tag_on_failure => ["_elapsedmin_failure_if1"]
  }

  grok{
     patterns_dir => ["./patterns"]
     match => { "Elapsed_time" => "\s(?<elapsed_sec>[0-9]*)\ssec"}
     tag_on_failure => ["_elapsedsec_failure_if1"]
  }
  mutate{
    add_field => {"elapsed_time_secs" => 0}
    convert => {
       "elapsed_min" => "integer"
       "elapsed_sec" => "integer"
       "elapsed_time_secs" => "integer"
    }
  }
  ruby {
    code => "event.set('elapsed_min', event.get('elapsed_min') * 60)"
  }
  if "_elapsedsec_failure_if1" in [tags]{
    ruby {
       code => "event.set('elapsed_time_secs', event.get('elapsed_min'))"
    }
  }else{
    ruby {
       code => "event.set('elapsed_time_secs', event.get('elapsed_min') + event.get('elapsed_sec'))"
    } 
  }

}else{
  mutate{
    add_tag => ["default_if"]
  }
  grok{
     patterns_dir => ["./patterns"]
     match => { "Elapsed_time" => "(?<elapsed_time_secs>[0-9]*)\ssec"}
     tag_on_failure => ["_elapsedsec_failure_else"]
  }
  mutate{
    convert => {
       "elapsed_time_secs" => "integer"
    }
  }
 }

##### Remove unnecessary fields #####

 mutate{
   remove_field => ["elapsed_min" , "elapsed_sec", "elapsed_hour", "elapsed_day"]
 }

########### Sometimes sofware compression has no percentatge and becomes null ####################

 if "No_software_compression" in [tags] {
   mutate{
    add_field => {"Software_Compression_Percentage" => 0.0}
     remove_tag => ["No_software_compression", "default_if", "min_if", "hour_if", "day_if", "_elapsedhour_failure_if3", "_elapsedmin_failure_if3", "_elapsedmin_failure_if2"]
   }
 }

####### Cleaning debugging tags ######

   mutate{
     remove_tag => ["No_software_compression", "default_if", "min_if", "hour_if", "day_if", "_elapsedhour_failure_if3", "_elapsedmin_failure_if3", "_elapsedmin_failure_if2", "_elapsedsec_failure_if1", "_elapsedsec_failure_if2", "_elapsedsec_failure_if3"]
   }


############ Converting all fields in to correct type #######################

 mutate{
  convert => {
    "Non-fatal_FD_errors" => "string"
    "Transfer_rate" => "string"
    "FD_Files_Written" => "string"
    "SD_Files_Written" => "string"
    "Rate" => "string"
    "Priority" => "string"
    "Snapshot" => "string"
    "Accurate" => "string"
    "Volume_Session_Time" => "string"
    "FD_MBytes_Written" => "string" 
    "SD_Errors" => "string"
    "Last_Volume_MBytes" => "string" 
    "SD_MBytes_Written" => "string"
  }
 }

 mutate{
  convert => {
    "Despooled_bytes" => "integer"
    "Encryption" => "boolean"
    "Volume_Session" => "integer"

    "Non-fatal_FD_errors" => "integer"
    "Transfer_rate" => "float"
    "FD_Files_Written" => "integer"
    "SD_Files_Written" => "integer"
    "Rate" => "float"
    "Priority" => "integer"
    "Snapshot" => "boolean"
    "Accurate" => "boolean"
    "Volume_Session_Time" => "integer"
    "FD_MBytes_Written" => "float" 
    "SD_Errors" => "integer"
    "Last_Volume_MBytes" => "float"
    "SD_MBytes_Written" => "float" 
    "Software_Compression_Percentage" => "float"
  }
 }

############# Establishing correct @timestamp with End_time #############

date {
  match => ["End_time", "dd-MMM-yyyy HH:mm:ss"]
  target => "@timestamp"
  locale => "en"
}

########### Removing parsed log message #############

 mutate{
  remove_field => ["message"]
 }
}

########## Elasticsearch classic output and stdout output for debugging ###############

output {
  elasticsearch {
    index => "bacula-%{+YYYY.MM.dd}"
    hosts => ["localhost:9200"]
  }
  #stdout { }
}
